{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df6398a9",
   "metadata": {},
   "source": [
    "### ü§ñ **Interactive Multi-LLM Interface with Gradio**\n",
    "\n",
    "This project implements a unified web interface that allows users to interact with three of the most powerful Large Language Models (LLMs) on the market: GPT-40-mini (OpenAI), Claude 3 Haiku (Anthropic), and Gemini 2.5 Flash (Google).\n",
    "\n",
    "The application uses the Gradio library to create the visual frontend and leverages the streaming capabilities of each API, allowing responses to be generated text-to-text in real time, thus improving the user experience. The code is designed in a modular fashion, with specific functions to handle the connection with each AI provider independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "30aad3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "import anthropic as ant\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b959fe",
   "metadata": {},
   "source": [
    "##### API Key Management and Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8ed6caa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AI\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "API_KEYS = {\n",
    "    \"OpenAI\": (\"OPENAI_API_KEY\", 8),\n",
    "    \"Anthropic\": (\"ANTHROPIC_API_KEY\", 7),\n",
    "    \"Google\": (\"GOOGLE_API_KEY\", 2)\n",
    "}\n",
    "\n",
    "keys = {label: os.getenv(env) for label, (env, _) in API_KEYS.items()}\n",
    "\n",
    "def check_key(label, prefix_len):\n",
    "    key = keys[label]\n",
    "    if key:\n",
    "        print(f\"{label} API Key exists and begins {key[:prefix_len]}\")\n",
    "    else:\n",
    "        print(f\"{label} API Key not set (optional)\")\n",
    "\n",
    "for label, (env_name, prefix_len) in API_KEYS.items():\n",
    "    check_key(label, prefix_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae04855",
   "metadata": {},
   "source": [
    "##### Client and Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2a2ae87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai  = OpenAI(api_key=keys[\"OpenAI\"])\n",
    "anthropic = ant.Anthropic(api_key=keys[\"Anthropic\"])\n",
    "genai.configure(api_key=keys[\"Google\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "626b341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "gemini_model = \"models/gemini-2.5-flash\"\n",
    "max_tokens = 5120\n",
    "system_prompt = \"You are a helpful assistant.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61da9219",
   "metadata": {},
   "source": [
    "##### Generation Function for OpenAI (GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1a243186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    result = \"\"\n",
    "    #display_handle = display(Markdown(\"‚è≥ _Generando..._\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "        #display_handle.update(Markdown(result))\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1e594325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object call_gpt at 0x0000020B7BBCC640>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt(\"What is the date today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5710e665",
   "metadata": {},
   "source": [
    "##### Generation Function for Anthropic (Claude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f6c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    response = anthropic.messages.stream(\n",
    "        model=claude_model,\n",
    "        system=system_prompt,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    accumulated_text = \"\"\n",
    "    #display_handle = display(Markdown(\"‚è≥ Generando respuesta...\"), display_id=True)\n",
    "    with response as stream:\n",
    "        for text in stream.text_stream:\n",
    "            accumulated_text += text\n",
    "            #display_handle.update(Markdown(accumulated_text))\n",
    "            yield accumulated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea622ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_claude(\"Explain the Transformer architecture to an aspiring AI engineer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac98ec1",
   "metadata": {},
   "source": [
    "##### Generation function for Google (Gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5c0b99d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini(prompt):\n",
    "    config = genai.types.GenerationConfig(\n",
    "        max_output_tokens=max_tokens,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=gemini_model,\n",
    "        system_instruction=system_prompt\n",
    "    )\n",
    "\n",
    "    response = model.generate_content(prompt, stream=True, generation_config=config)\n",
    "    accumulated_text = \"\"\n",
    "    #display_handle = display(Markdown(\"‚è≥ Generando respuesta...\"), display_id=True)\n",
    "    for chunk in response:\n",
    "        if chunk.text:\n",
    "            accumulated_text += chunk.text\n",
    "            #display_handle.update(Markdown(accumulated_text))\n",
    "            yield accumulated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2ea3d080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object call_gemini at 0x0000020B7BBCDA40>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gemini(\"Explain the Transformer architecture to an aspiring AI engineer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bad9f15",
   "metadata": {},
   "source": [
    "### Model Selection Logic (Dispatcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a70efb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_model(prompt, model_name):\n",
    "    models = {\n",
    "        \"GPT\": call_gpt,\n",
    "        \"Claude\": call_claude,\n",
    "        \"Gemini\": call_gemini\n",
    "    }\n",
    "\n",
    "    if model_name not in models:\n",
    "        raise ValueError(f\"Unknown model '{model_name}'. Available: {list(models.keys())}\")\n",
    "\n",
    "    yield from models[model_name](prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "45dda0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object stream_model at 0x0000020B7C36EF80>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_model(\"Dime un chiste\", \"Ollama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c99f271",
   "metadata": {},
   "source": [
    "### Building and Testing the Interface with Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b38714a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shout(text):\n",
    "    return text.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c4cd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "shout(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d605e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\", flagging_mode=\"never\").launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4b9b02",
   "metadata": {},
   "source": [
    "##### Gradio dark mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450c720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "force_dark_mode = \"\"\"\n",
    "(function() {\n",
    "    const url = new URL(window.location);\n",
    "    if (url.searchParams.get('__theme') !== 'dark') {\n",
    "        url.searchParams.set('__theme', 'dark');\n",
    "        window.location.href = url.href;\n",
    "    }\n",
    "})();\n",
    "\"\"\"\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=shout, \n",
    "    inputs=\"textbox\", \n",
    "    outputs=\"textbox\", \n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "\n",
    "demo.launch(share=True, js=force_dark_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797cffba",
   "metadata": {},
   "source": [
    "### Application Launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "4ffd5140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7900\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7900/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_input = gr.Textbox(label=\"Your message:\", lines=7)\n",
    "model_selector = gr.Dropdown([\"GPT\", \"Claude\", \"Gemini\"], label=\"Select model\", value=\"GPT\")\n",
    "message_output = gr.Markdown(label=\"Response:\")\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=stream_model,\n",
    "    title=\"LLMs\", \n",
    "    inputs=[message_input, model_selector], \n",
    "    outputs=[message_output], \n",
    "    examples=[\n",
    "       [\"Explain the Transformer architecture to a layperson\", \"GPT\"],\n",
    "       [\"Explain the Transformer architecture to an aspiring AI engineer\", \"Claude\"],\n",
    "       [\"Explain the Transformer architecture to an Software engineer\", \"Gemini\"]\n",
    "    ],\n",
    "    flagging_mode=\"never\"\n",
    "    )\n",
    "view.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
