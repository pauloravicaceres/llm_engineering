{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ad8048d",
   "metadata": {},
   "source": [
    "### Exploring and Using Frontier Model APIs\n",
    "\n",
    "##### Suggested Description\n",
    "\n",
    "This project aims to connect with and experiment with advanced language models (‚ÄúFrontier Models‚Äù) through their official APIs. After using various models via their chat interfaces and working with the OpenAI API in the first week, we will now expand our scope by interacting programmatically with multiple providers, such as Anthropic, Gemini, and others, provided we have their access credentials.\n",
    "\n",
    "The goal is to send queries to different models, compare their responses, and explore the capabilities, differences, and behaviors of each API. Integration with additional providers is entirely optional, allowing each participant to customize their environment according to the tools they have access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a722840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "import anthropic as ant\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1210883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AI\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "API_KEYS = {\n",
    "    \"OpenAI\": (\"OPENAI_API_KEY\", 8),\n",
    "    \"Anthropic\": (\"ANTHROPIC_API_KEY\", 7),\n",
    "    \"Google\": (\"GOOGLE_API_KEY\", 2)\n",
    "}\n",
    "\n",
    "keys = {label: os.getenv(env) for label, (env, _) in API_KEYS.items()}\n",
    "\n",
    "def check_key(label, prefix_len):\n",
    "    key = keys[label]\n",
    "    if key:\n",
    "        print(f\"{label} API Key exists and begins {key[:prefix_len]}\")\n",
    "    else:\n",
    "        print(f\"{label} API Key not set (optional)\")\n",
    "\n",
    "for label, (env_name, prefix_len) in API_KEYS.items():\n",
    "    check_key(label, prefix_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a53141e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai  = OpenAI(api_key=keys[\"OpenAI\"])\n",
    "anthropic = ant.Anthropic(api_key=keys[\"Anthropic\"])\n",
    "genai.configure(api_key=keys[\"Google\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4243823",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 1024\n",
    "MODEL = \"gpt-4.1-mini\"\n",
    "SYSTEM_PROMPT = \"You are a very helpful assistant\"\n",
    "USER_PROMPT = [\n",
    "    { \"role\": \"user\", \"content\": \"How can I decide if a business problem is suitable for an LLM solution? Answer in Markdown\" }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4811cdbe",
   "metadata": {},
   "source": [
    "## The Internal Structure of the Message History\n",
    "\n",
    "Originally, `gradio` expected to receive a function called:\n",
    "\n",
    "`chat(message, history)`\n",
    "\n",
    "This function had to receive `history` in a specific format, which we had to assign to the OpenAI format before making the call:\n",
    "\n",
    "```json\n",
    "[\n",
    "{\"role\": \"system\", \"content\": \"system message here\"},\n",
    "{\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "{\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "{\"role\": \"user\", \"content\": \"the new user prompt\"}\n",
    "]\n",
    "```\n",
    "### Gradio has been updated! üöÄ\n",
    "\n",
    "Now it will pass `history` in OpenAI's **exact** format, perfect for sending directly to the API. So our work just got easier!\n",
    "\n",
    "#### üìã Action Plan:\n",
    "\n",
    "We'll write a function `chat(message, history)` where:\n",
    "\n",
    "* **`message`**: This is the current message to use.\n",
    "\n",
    "* **`history`**: This is the previous conversation (already in OpenAI-compatible format).\n",
    "\n",
    "We'll combine the **system message**, the **history**, and the **last message**, and then call OpenAI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b0f13b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    history = [{\"role\":h[\"role\"], \"content\":h[\"content\"]} for h in history]\n",
    "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e36daff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    history = [{\"role\":h[\"role\"], \"content\":h[\"content\"]} for h in history]\n",
    "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    print(\"history:\", history)\n",
    "    print(\"messages:\", messages)\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5df017",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea947fc4",
   "metadata": {},
   "source": [
    "## OK let's keep going!\n",
    "\n",
    "Using a system message to add context, and to give an example answer.. this is \"one shot prompting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f368fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant in a clothes store. You should try to gently encourage \\\n",
    "the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. \\\n",
    "For example, if the customer says 'I'm looking to buy a hat', \\\n",
    "you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'\\\n",
    "Encourage the customer to buy hats if they are unsure what to get.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37e4abc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    history = [{\"role\":h[\"role\"], \"content\":h[\"content\"]} for h in history]\n",
    "    relevant_system_message = system_message\n",
    "    if 'belt' in message.lower():\n",
    "        relevant_system_message += \" The store does not sell belts; if you are asked for belts, be sure to point out other items on sale.\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": relevant_system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf449fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3b5368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message += \"\\nIf the customer asks for shoes, you should respond that shoes are not on sale today, \\\n",
    "but remind the customer to look at hats!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cde81d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    history = [{\"role\":h[\"role\"], \"content\":h[\"content\"]} for h in history]\n",
    "    relevant_system_message = system_message\n",
    "    if re.search(r\"\\bbelts?\\b\", message, re.IGNORECASE):\n",
    "        relevant_system_message += \" The store does not sell belts; if you are asked for belts, be sure to point out other items on sale.\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": relevant_system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde95e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c1c2ad",
   "metadata": {},
   "source": [
    "# Project - PPTX summarizer AI Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ad8a2932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pptx import Presentation\n",
    "import json\n",
    "\n",
    "class PptxSummary:\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "\n",
    "    def summary(self):\n",
    "        # Load an existing PowerPoint\n",
    "        prs = Presentation(self.name)\n",
    "        newtext = \"\"\n",
    "\n",
    "        # print(prs)\n",
    "        # Loop over all slides\n",
    "        for i, slide in enumerate(prs.slides, start=1):\n",
    "            # print(f\"\\n--- Slide {i} ---\")\n",
    "            newtext += f\"\\n\\n--- Slide {i} ---\"\n",
    "            \n",
    "            # Loop over shapes (text boxes, titles, placeholders, etc.)\n",
    "            for shape in slide.shapes:\n",
    "                if shape.has_text_frame:  # Only shapes that can contain text\n",
    "                    for paragraph in shape.text_frame.paragraphs:\n",
    "                        # Collect text from each run in the paragraph\n",
    "                        text = \"\".join(run.text for run in paragraph.runs)\n",
    "                        # print(text)\n",
    "                        newtext+= \"\\n\"\n",
    "                        newtext += text\n",
    "        # print(newtext)\n",
    "        return newtext\n",
    "\n",
    "        \n",
    "system_message = \"You are a helpful assistant for a company and you can summarize the given topic based on the given pptx name. \"\n",
    "system_message += \"Give short, courteous answers, no more than 10 sentence. \"\n",
    "system_message += \"Always be accurate. If the presentation .pptx file does not exist, say so. Respond in markdown.\"\n",
    "\n",
    "def user_message(path):\n",
    "    ppt = PptxSummary(path)\n",
    "    summarization_message = ppt.summary()\n",
    "    message = \"You need to summarize the a pptx file.\"\n",
    "    message += f\"The context of that pptx file is here: {summarization_message}\"\n",
    "    message += \"Give the concise information in small paragraphs. \"\n",
    "    return message\n",
    "\n",
    "def pptx_summary(path):\n",
    "    if os.path.exists(path):\n",
    "        result = \"The file does not exist\"\n",
    "        response = openai.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_message(path)}\n",
    "              ],\n",
    "        )\n",
    "        result = response.choices[0].message.content\n",
    "        return result\n",
    "    else:\n",
    "        return \"The file does not exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "906142a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The presentation titled \"Miguel Grau\" appears to focus on an individual named Miguel Grau. Unfortunately, the information provided is minimal. The second slide mentions a birth date of \"30 de febrero de 1987,\" which is not a valid date, indicating a possible error. No further details or context are available from the slides provided. If you have more slides or information, I can help provide a more detailed summary.\n"
     ]
    }
   ],
   "source": [
    "print(pptx_summary(\"data/conversational_chatbot/Miguel_Grau.pptx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "21774108",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_function = {\n",
    "    \"name\": \"pptx_summary\",\n",
    "    \"description\": \"Get the summary for the given pptx file, you need to call this function, if user asks for a pptx file, if it is outside of the pptx file, tell that you do not know.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"path\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The name of the presentation\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"path\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f337856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\"type\": \"function\", \"function\": summary_function}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "86a341ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_tool_call(message):\n",
    "    tool_call = message.tool_calls[0]\n",
    "    print(f\"tool_call: {tool_call}\")\n",
    "    arguments = json.loads(tool_call.function.arguments)\n",
    "    print(f\"arguments: {arguments}\")\n",
    "    path = f\"data/conversational_chatbot/{arguments.get('path')}\"\n",
    "    print(f\"path: {path}\")\n",
    "    summary = pptx_summary(path)\n",
    "    # print(f\"price: {price}\")\n",
    "    response = {\n",
    "        \"role\": \"tool\",\n",
    "        \"content\": json.dumps({\"path\": path,\"summary\": summary}),\n",
    "        \"tool_call_id\": tool_call.id\n",
    "    }\n",
    "    print(f\"response: {response}\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "50e86cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=MODEL, messages=messages, tools=tools)\n",
    "    print(f\"response 00: {response}\")\n",
    "    print(f\"response 00xx: {response.choices[0].message.content}\")\n",
    "\n",
    "    if response.choices[0].finish_reason==\"tool_calls\":\n",
    "        message = response.choices[0].message\n",
    "        print(f\"message: {message}\")\n",
    "        response = handle_tool_call(message)\n",
    "        print(f\"response 01: {response}\")\n",
    "        messages.append(message)\n",
    "        messages.append(response)\n",
    "        response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "        print(f\"response 02: {response}\")\n",
    "        print(f\"response 03: {response.choices[0].message.content}\")\n",
    "            \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2b9e8716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7879\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7879/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response 00: ChatCompletion(id='chatcmpl-CllhT7BhlUGstfjPHFaxs3u3NPuHx', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call_IJ8AxNQ6AjYQJricVSRBolRF', function=Function(arguments='{\"path\":\"Miguel_Grau.pptx\"}', name='pptx_summary'), type='function')]))], created=1765500811, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_7abc656409', usage=CompletionUsage(completion_tokens=22, prompt_tokens=153, total_tokens=175, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "response 00xx: None\n",
      "message: ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call_IJ8AxNQ6AjYQJricVSRBolRF', function=Function(arguments='{\"path\":\"Miguel_Grau.pptx\"}', name='pptx_summary'), type='function')])\n",
      "tool_call: ChatCompletionMessageFunctionToolCall(id='call_IJ8AxNQ6AjYQJricVSRBolRF', function=Function(arguments='{\"path\":\"Miguel_Grau.pptx\"}', name='pptx_summary'), type='function')\n",
      "arguments: {'path': 'Miguel_Grau.pptx'}\n",
      "path: data/conversational_chatbot/Miguel_Grau.pptx\n",
      "response: {'role': 'tool', 'content': '{\"path\": \"data/conversational_chatbot/Miguel_Grau.pptx\", \"summary\": \"The provided pptx file appears to contain information about Miguel Grau. However, an error is present regarding his birth date, since February 30th does not exist. This suggests the date might be incorrect or a typographical mistake. Other than his name and the questionable birth date, no further details about Miguel Grau are available in the slides.\"}', 'tool_call_id': 'call_IJ8AxNQ6AjYQJricVSRBolRF'}\n",
      "response 01: {'role': 'tool', 'content': '{\"path\": \"data/conversational_chatbot/Miguel_Grau.pptx\", \"summary\": \"The provided pptx file appears to contain information about Miguel Grau. However, an error is present regarding his birth date, since February 30th does not exist. This suggests the date might be incorrect or a typographical mistake. Other than his name and the questionable birth date, no further details about Miguel Grau are available in the slides.\"}', 'tool_call_id': 'call_IJ8AxNQ6AjYQJricVSRBolRF'}\n",
      "response 02: ChatCompletion(id='chatcmpl-CllhXGhoyScj9XAshNcx5kmNZ8D3F', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The presentation titled \"Miguel_Grau.pptx\" contains information about Miguel Grau. However, it includes an incorrect birth date (February 30th), which is not a valid date. Apart from his name and the questionable birth date, no additional details about Miguel Grau are provided in the slides.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1765500815, model='gpt-4.1-mini-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_7abc656409', usage=CompletionUsage(completion_tokens=61, prompt_tokens=199, total_tokens=260, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "response 03: The presentation titled \"Miguel_Grau.pptx\" contains information about Miguel Grau. However, it includes an incorrect birth date (February 30th), which is not a valid date. Apart from his name and the questionable birth date, no additional details about Miguel Grau are provided in the slides.\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat).launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
