{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ad8048d",
   "metadata": {},
   "source": [
    "### Exploring and Using Frontier Model APIs\n",
    "\n",
    "##### Suggested Description\n",
    "\n",
    "This project aims to connect with and experiment with advanced language models (“Frontier Models”) through their official APIs. After using various models via their chat interfaces and working with the OpenAI API in the first week, we will now expand our scope by interacting programmatically with multiple providers, such as Anthropic, Gemini, and others, provided we have their access credentials.\n",
    "\n",
    "The goal is to send queries to different models, compare their responses, and explore the capabilities, differences, and behaviors of each API. Integration with additional providers is entirely optional, allowing each participant to customize their environment according to the tools they have access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a722840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "import anthropic as ant\n",
    "import google.generativeai as genai\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1210883",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "API_KEYS = {\n",
    "    \"OpenAI\": (\"OPENAI_API_KEY\", 8),\n",
    "    \"Anthropic\": (\"ANTHROPIC_API_KEY\", 7),\n",
    "    \"Google\": (\"GOOGLE_API_KEY\", 2),\n",
    "    \"DeepSeek\": (\"DEEPSEEK_API_KEY\", 3),\n",
    "    \"Groq\": (\"GROQ_API_KEY\", 4),\n",
    "    \"Grok\": (\"GROK_API_KEY\", 4),\n",
    "    \"OpenRouter\": (\"OPENROUTER_API_KEY\", 3),\n",
    "}\n",
    "\n",
    "keys = {label: os.getenv(env) for label, (env, _) in API_KEYS.items()}\n",
    "\n",
    "def check_key(label, prefix_len):\n",
    "    key = keys[label]\n",
    "    if key:\n",
    "        print(f\"{label} API Key exists and begins {key[:prefix_len]}\")\n",
    "    else:\n",
    "        print(f\"{label} API Key not set (optional)\")\n",
    "\n",
    "for label, (env_name, prefix_len) in API_KEYS.items():\n",
    "    check_key(label, prefix_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53141e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "\n",
    "# For Gemini, DeepSeek and Groq, we can use the OpenAI python client\n",
    "# Because Google and DeepSeek have endpoints compatible with OpenAI\n",
    "# And OpenAI allows you to change the base_url\n",
    "\n",
    "urls = {\n",
    "    \"Anthropic\": \"https://api.anthropic.com/v1/\",\n",
    "    \"Gemini\": \"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    "    \"DeepSeek\": \"https://api.deepseek.com\",\n",
    "    \"Groq\": \"https://api.groq.com/openai/v1\",\n",
    "    \"Grok\": \"https://api.x.ai/v1\",\n",
    "    \"OpenRouter\": \"https://openrouter.ai/api/v1\",\n",
    "    \"ollama\": \"http://localhost:11434/v1\",\n",
    "}\n",
    "\n",
    "openai  = OpenAI(api_key=keys[\"OpenAI\"])\n",
    "#anthropic = OpenAI(api_key=keys[\"Anthropic\"], base_url=urls[\"Anthropic\"])\n",
    "anthropic = ant.Anthropic(api_key=keys[\"Anthropic\"])\n",
    "#gemini    = OpenAI(api_key=keys[\"Google\"],    base_url=urls[\"Gemini\"])\n",
    "genai.configure(api_key=keys[\"Google\"])\n",
    "deepseek  = OpenAI(api_key=keys[\"DeepSeek\"],  base_url=urls[\"DeepSeek\"])\n",
    "groq      = OpenAI(api_key=keys[\"Groq\"],      base_url=urls[\"Groq\"])\n",
    "grok      = OpenAI(api_key=keys[\"Grok\"],      base_url=urls[\"Grok\"])\n",
    "openrouter = OpenAI(api_key=keys[\"OpenRouter\"], base_url=urls[\"OpenRouter\"])\n",
    "ollama    = OpenAI(api_key=\"ollama\", base_url=urls[\"ollama\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4243823",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 1024\n",
    "SYSTEM_PROMPT = \"You are a helpful assistant who responds in Markdown\"\n",
    "USER_PROMPT = [\n",
    "    { \"role\": \"user\", \"content\": \"How can I decide if a business problem is suitable for an LLM solution? Answer in Markdown\" }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa485cf",
   "metadata": {},
   "source": [
    "##### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93729ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_prompt = [{ \"role\": \"system\", \"content\": SYSTEM_PROMPT }] + USER_PROMPT\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=full_prompt,\n",
    "    temperature=0.2,\n",
    "    stream=True\n",
    ")\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"⏳ _Generando..._\"), display_id=True)\n",
    "for chunk in response:\n",
    "    fragment = chunk.choices[0].delta.content or \"\"\n",
    "    reply += fragment\n",
    "    clean_reply = reply\n",
    "    if clean_reply.startswith(\"```\"):\n",
    "        clean_reply = clean_reply.replace(\"```markdown\", \"\").replace(\"```\", \"\")\n",
    "    display_handle.update(Markdown(clean_reply))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86147bde",
   "metadata": {},
   "source": [
    "##### Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e12fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = anthropic.messages.create(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    system=SYSTEM_PROMPT,\n",
    "    messages=USER_PROMPT,\n",
    "    temperature=0.2\n",
    ")\n",
    "display(Markdown(response.content[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baafc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = anthropic.messages.stream(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    system=SYSTEM_PROMPT,\n",
    "    messages=USER_PROMPT,\n",
    "    temperature=0.9\n",
    ")\n",
    "accumulated_text = \"\"\n",
    "display_handle = display(Markdown(\"⏳ Generando respuesta...\"), display_id=True)\n",
    "with response as stream:\n",
    "    for text in stream.text_stream:\n",
    "        accumulated_text += text\n",
    "        display_handle.update(Markdown(accumulated_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee946b57",
   "metadata": {},
   "source": [
    "##### Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75338a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Generation Models:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>display_name</th>\n",
       "      <th>input_limit</th>\n",
       "      <th>output_limit</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>models/gemini-2.5-flash</td>\n",
       "      <td>Gemini 2.5 Flash</td>\n",
       "      <td>1048576</td>\n",
       "      <td>65536</td>\n",
       "      <td>Stable version of Gemini 2.5 Flash, our mid-size multimodal model that supports up to 1 million tokens, released in June of 2025.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>models/gemini-2.5-pro</td>\n",
       "      <td>Gemini 2.5 Pro</td>\n",
       "      <td>1048576</td>\n",
       "      <td>65536</td>\n",
       "      <td>Stable release (June 17th, 2025) of Gemini 2.5 Pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>models/gemini-2.0-flash-exp</td>\n",
       "      <td>Gemini 2.0 Flash Experimental</td>\n",
       "      <td>1048576</td>\n",
       "      <td>8192</td>\n",
       "      <td>Gemini 2.0 Flash Experimental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>models/gemini-2.0-flash</td>\n",
       "      <td>Gemini 2.0 Flash</td>\n",
       "      <td>1048576</td>\n",
       "      <td>8192</td>\n",
       "      <td>Gemini 2.0 Flash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>models/gemini-2.0-flash-001</td>\n",
       "      <td>Gemini 2.0 Flash 001</td>\n",
       "      <td>1048576</td>\n",
       "      <td>8192</td>\n",
       "      <td>Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>models/gemini-2.0-flash-exp-image-generation</td>\n",
       "      <td>Gemini 2.0 Flash (Image Generation) Experimental</td>\n",
       "      <td>1048576</td>\n",
       "      <td>8192</td>\n",
       "      <td>Gemini 2.0 Flash (Image Generation) Experimental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>models/gemini-2.0-flash-lite-001</td>\n",
       "      <td>Gemini 2.0 Flash-Lite 001</td>\n",
       "      <td>1048576</td>\n",
       "      <td>8192</td>\n",
       "      <td>Stable version of Gemini 2.0 Flash-Lite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>models/gemini-2.0-flash-lite</td>\n",
       "      <td>Gemini 2.0 Flash-Lite</td>\n",
       "      <td>1048576</td>\n",
       "      <td>8192</td>\n",
       "      <td>Gemini 2.0 Flash-Lite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>models/gemini-2.0-flash-lite-preview-02-05</td>\n",
       "      <td>Gemini 2.0 Flash-Lite Preview 02-05</td>\n",
       "      <td>1048576</td>\n",
       "      <td>8192</td>\n",
       "      <td>Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>models/gemini-2.0-flash-lite-preview</td>\n",
       "      <td>Gemini 2.0 Flash-Lite Preview</td>\n",
       "      <td>1048576</td>\n",
       "      <td>8192</td>\n",
       "      <td>Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>models/gemini-exp-1206</td>\n",
       "      <td>Gemini Experimental 1206</td>\n",
       "      <td>1048576</td>\n",
       "      <td>65536</td>\n",
       "      <td>Experimental release (March 25th, 2025) of Gemini 2.5 Pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>models/gemini-2.5-flash-preview-tts</td>\n",
       "      <td>Gemini 2.5 Flash Preview TTS</td>\n",
       "      <td>8192</td>\n",
       "      <td>16384</td>\n",
       "      <td>Gemini 2.5 Flash Preview TTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>models/gemini-2.5-pro-preview-tts</td>\n",
       "      <td>Gemini 2.5 Pro Preview TTS</td>\n",
       "      <td>8192</td>\n",
       "      <td>16384</td>\n",
       "      <td>Gemini 2.5 Pro Preview TTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>models/gemma-3-1b-it</td>\n",
       "      <td>Gemma 3 1B</td>\n",
       "      <td>32768</td>\n",
       "      <td>8192</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>models/gemma-3-4b-it</td>\n",
       "      <td>Gemma 3 4B</td>\n",
       "      <td>32768</td>\n",
       "      <td>8192</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>models/gemma-3-12b-it</td>\n",
       "      <td>Gemma 3 12B</td>\n",
       "      <td>32768</td>\n",
       "      <td>8192</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>models/gemma-3-27b-it</td>\n",
       "      <td>Gemma 3 27B</td>\n",
       "      <td>131072</td>\n",
       "      <td>8192</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>models/gemma-3n-e4b-it</td>\n",
       "      <td>Gemma 3n E4B</td>\n",
       "      <td>8192</td>\n",
       "      <td>2048</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>models/gemma-3n-e2b-it</td>\n",
       "      <td>Gemma 3n E2B</td>\n",
       "      <td>8192</td>\n",
       "      <td>2048</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>models/gemini-flash-latest</td>\n",
       "      <td>Gemini Flash Latest</td>\n",
       "      <td>1048576</td>\n",
       "      <td>65536</td>\n",
       "      <td>Latest release of Gemini Flash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>models/gemini-flash-lite-latest</td>\n",
       "      <td>Gemini Flash-Lite Latest</td>\n",
       "      <td>1048576</td>\n",
       "      <td>65536</td>\n",
       "      <td>Latest release of Gemini Flash-Lite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>models/gemini-pro-latest</td>\n",
       "      <td>Gemini Pro Latest</td>\n",
       "      <td>1048576</td>\n",
       "      <td>65536</td>\n",
       "      <td>Latest release of Gemini Pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>models/gemini-2.5-flash-lite</td>\n",
       "      <td>Gemini 2.5 Flash-Lite</td>\n",
       "      <td>1048576</td>\n",
       "      <td>65536</td>\n",
       "      <td>Stable version of Gemini 2.5 Flash-Lite, released in July of 2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>models/gemini-2.5-flash-image-preview</td>\n",
       "      <td>Nano Banana</td>\n",
       "      <td>32768</td>\n",
       "      <td>32768</td>\n",
       "      <td>Gemini 2.5 Flash Preview Image</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>models/gemini-2.5-flash-image</td>\n",
       "      <td>Nano Banana</td>\n",
       "      <td>32768</td>\n",
       "      <td>32768</td>\n",
       "      <td>Gemini 2.5 Flash Preview Image</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>models/gemini-2.5-flash-preview-09-2025</td>\n",
       "      <td>Gemini 2.5 Flash Preview Sep 2025</td>\n",
       "      <td>1048576</td>\n",
       "      <td>65536</td>\n",
       "      <td>Gemini 2.5 Flash Preview Sep 2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>models/gemini-2.5-flash-lite-preview-09-2025</td>\n",
       "      <td>Gemini 2.5 Flash-Lite Preview Sep 2025</td>\n",
       "      <td>1048576</td>\n",
       "      <td>65536</td>\n",
       "      <td>Preview release (Septempber 25th, 2025) of Gemini 2.5 Flash-Lite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>models/gemini-3-pro-preview</td>\n",
       "      <td>Gemini 3 Pro Preview</td>\n",
       "      <td>1048576</td>\n",
       "      <td>65536</td>\n",
       "      <td>Gemini 3 Pro Preview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>models/gemini-3-pro-image-preview</td>\n",
       "      <td>Nano Banana Pro</td>\n",
       "      <td>131072</td>\n",
       "      <td>32768</td>\n",
       "      <td>Gemini 3 Pro Image Preview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>models/nano-banana-pro-preview</td>\n",
       "      <td>Nano Banana Pro</td>\n",
       "      <td>131072</td>\n",
       "      <td>32768</td>\n",
       "      <td>Gemini 3 Pro Image Preview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>models/gemini-robotics-er-1.5-preview</td>\n",
       "      <td>Gemini Robotics-ER 1.5 Preview</td>\n",
       "      <td>1048576</td>\n",
       "      <td>65536</td>\n",
       "      <td>Gemini Robotics-ER 1.5 Preview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>models/gemini-2.5-computer-use-preview-10-2025</td>\n",
       "      <td>Gemini 2.5 Computer Use Preview 10-2025</td>\n",
       "      <td>131072</td>\n",
       "      <td>65536</td>\n",
       "      <td>Gemini 2.5 Computer Use Preview 10-2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>models/deep-research-pro-preview-12-2025</td>\n",
       "      <td>Deep Research Pro Preview (Dec-12-2025)</td>\n",
       "      <td>131072</td>\n",
       "      <td>65536</td>\n",
       "      <td>Preview release (December 12th, 2025) of Deep Research Pro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              name  \\\n",
       "0                          models/gemini-2.5-flash   \n",
       "1                            models/gemini-2.5-pro   \n",
       "2                      models/gemini-2.0-flash-exp   \n",
       "3                          models/gemini-2.0-flash   \n",
       "4                      models/gemini-2.0-flash-001   \n",
       "5     models/gemini-2.0-flash-exp-image-generation   \n",
       "6                 models/gemini-2.0-flash-lite-001   \n",
       "7                     models/gemini-2.0-flash-lite   \n",
       "8       models/gemini-2.0-flash-lite-preview-02-05   \n",
       "9             models/gemini-2.0-flash-lite-preview   \n",
       "10                          models/gemini-exp-1206   \n",
       "11             models/gemini-2.5-flash-preview-tts   \n",
       "12               models/gemini-2.5-pro-preview-tts   \n",
       "13                            models/gemma-3-1b-it   \n",
       "14                            models/gemma-3-4b-it   \n",
       "15                           models/gemma-3-12b-it   \n",
       "16                           models/gemma-3-27b-it   \n",
       "17                          models/gemma-3n-e4b-it   \n",
       "18                          models/gemma-3n-e2b-it   \n",
       "19                      models/gemini-flash-latest   \n",
       "20                 models/gemini-flash-lite-latest   \n",
       "21                        models/gemini-pro-latest   \n",
       "22                    models/gemini-2.5-flash-lite   \n",
       "23           models/gemini-2.5-flash-image-preview   \n",
       "24                   models/gemini-2.5-flash-image   \n",
       "25         models/gemini-2.5-flash-preview-09-2025   \n",
       "26    models/gemini-2.5-flash-lite-preview-09-2025   \n",
       "27                     models/gemini-3-pro-preview   \n",
       "28               models/gemini-3-pro-image-preview   \n",
       "29                  models/nano-banana-pro-preview   \n",
       "30           models/gemini-robotics-er-1.5-preview   \n",
       "31  models/gemini-2.5-computer-use-preview-10-2025   \n",
       "32        models/deep-research-pro-preview-12-2025   \n",
       "\n",
       "                                        display_name  input_limit  \\\n",
       "0                                   Gemini 2.5 Flash      1048576   \n",
       "1                                     Gemini 2.5 Pro      1048576   \n",
       "2                      Gemini 2.0 Flash Experimental      1048576   \n",
       "3                                   Gemini 2.0 Flash      1048576   \n",
       "4                               Gemini 2.0 Flash 001      1048576   \n",
       "5   Gemini 2.0 Flash (Image Generation) Experimental      1048576   \n",
       "6                          Gemini 2.0 Flash-Lite 001      1048576   \n",
       "7                              Gemini 2.0 Flash-Lite      1048576   \n",
       "8                Gemini 2.0 Flash-Lite Preview 02-05      1048576   \n",
       "9                      Gemini 2.0 Flash-Lite Preview      1048576   \n",
       "10                          Gemini Experimental 1206      1048576   \n",
       "11                      Gemini 2.5 Flash Preview TTS         8192   \n",
       "12                        Gemini 2.5 Pro Preview TTS         8192   \n",
       "13                                        Gemma 3 1B        32768   \n",
       "14                                        Gemma 3 4B        32768   \n",
       "15                                       Gemma 3 12B        32768   \n",
       "16                                       Gemma 3 27B       131072   \n",
       "17                                      Gemma 3n E4B         8192   \n",
       "18                                      Gemma 3n E2B         8192   \n",
       "19                               Gemini Flash Latest      1048576   \n",
       "20                          Gemini Flash-Lite Latest      1048576   \n",
       "21                                 Gemini Pro Latest      1048576   \n",
       "22                             Gemini 2.5 Flash-Lite      1048576   \n",
       "23                                       Nano Banana        32768   \n",
       "24                                       Nano Banana        32768   \n",
       "25                 Gemini 2.5 Flash Preview Sep 2025      1048576   \n",
       "26            Gemini 2.5 Flash-Lite Preview Sep 2025      1048576   \n",
       "27                              Gemini 3 Pro Preview      1048576   \n",
       "28                                   Nano Banana Pro       131072   \n",
       "29                                   Nano Banana Pro       131072   \n",
       "30                    Gemini Robotics-ER 1.5 Preview      1048576   \n",
       "31           Gemini 2.5 Computer Use Preview 10-2025       131072   \n",
       "32           Deep Research Pro Preview (Dec-12-2025)       131072   \n",
       "\n",
       "    output_limit  \\\n",
       "0          65536   \n",
       "1          65536   \n",
       "2           8192   \n",
       "3           8192   \n",
       "4           8192   \n",
       "5           8192   \n",
       "6           8192   \n",
       "7           8192   \n",
       "8           8192   \n",
       "9           8192   \n",
       "10         65536   \n",
       "11         16384   \n",
       "12         16384   \n",
       "13          8192   \n",
       "14          8192   \n",
       "15          8192   \n",
       "16          8192   \n",
       "17          2048   \n",
       "18          2048   \n",
       "19         65536   \n",
       "20         65536   \n",
       "21         65536   \n",
       "22         65536   \n",
       "23         32768   \n",
       "24         32768   \n",
       "25         65536   \n",
       "26         65536   \n",
       "27         65536   \n",
       "28         32768   \n",
       "29         32768   \n",
       "30         65536   \n",
       "31         65536   \n",
       "32         65536   \n",
       "\n",
       "                                                                                                                                   description  \n",
       "0            Stable version of Gemini 2.5 Flash, our mid-size multimodal model that supports up to 1 million tokens, released in June of 2025.  \n",
       "1                                                                                           Stable release (June 17th, 2025) of Gemini 2.5 Pro  \n",
       "2                                                                                                                Gemini 2.0 Flash Experimental  \n",
       "3                                                                                                                             Gemini 2.0 Flash  \n",
       "4   Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.  \n",
       "5                                                                                             Gemini 2.0 Flash (Image Generation) Experimental  \n",
       "6                                                                                                      Stable version of Gemini 2.0 Flash-Lite  \n",
       "7                                                                                                                        Gemini 2.0 Flash-Lite  \n",
       "8                                                                                Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite  \n",
       "9                                                                                Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite  \n",
       "10                                                                                   Experimental release (March 25th, 2025) of Gemini 2.5 Pro  \n",
       "11                                                                                                                Gemini 2.5 Flash Preview TTS  \n",
       "12                                                                                                                  Gemini 2.5 Pro Preview TTS  \n",
       "13                                                                                                                                              \n",
       "14                                                                                                                                              \n",
       "15                                                                                                                                              \n",
       "16                                                                                                                                              \n",
       "17                                                                                                                                              \n",
       "18                                                                                                                                              \n",
       "19                                                                                                              Latest release of Gemini Flash  \n",
       "20                                                                                                         Latest release of Gemini Flash-Lite  \n",
       "21                                                                                                                Latest release of Gemini Pro  \n",
       "22                                                                           Stable version of Gemini 2.5 Flash-Lite, released in July of 2025  \n",
       "23                                                                                                              Gemini 2.5 Flash Preview Image  \n",
       "24                                                                                                              Gemini 2.5 Flash Preview Image  \n",
       "25                                                                                                           Gemini 2.5 Flash Preview Sep 2025  \n",
       "26                                                                            Preview release (Septempber 25th, 2025) of Gemini 2.5 Flash-Lite  \n",
       "27                                                                                                                        Gemini 3 Pro Preview  \n",
       "28                                                                                                                  Gemini 3 Pro Image Preview  \n",
       "29                                                                                                                  Gemini 3 Pro Image Preview  \n",
       "30                                                                                                              Gemini Robotics-ER 1.5 Preview  \n",
       "31                                                                                                     Gemini 2.5 Computer Use Preview 10-2025  \n",
       "32                                                                                  Preview release (December 12th, 2025) of Deep Research Pro  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List the available Google models\n",
    "modelos = list(genai.list_models())\n",
    "\n",
    "datos_modelos = []\n",
    "for m in modelos:\n",
    "    if 'generateContent' in m.supported_generation_methods:\n",
    "        datos_modelos.append({\n",
    "            'name': m.name,\n",
    "            'display_name': m.display_name,\n",
    "            'input_limit': m.input_token_limit,\n",
    "            'output_limit': m.output_token_limit,\n",
    "            'description': m.description\n",
    "        })\n",
    "\n",
    "df_google = pd.DataFrame(datos_modelos)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "cols_a_mostrar = ['name', 'display_name', 'input_limit', 'output_limit', 'description']\n",
    "\n",
    "print(\"Google Generation Models:\")\n",
    "display(df_google[cols_a_mostrar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366163a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    model_name=\"models/gemini-2.5-flash\",\n",
    "    system_instruction=SYSTEM_PROMPT\n",
    ")\n",
    "prompt_text = USER_PROMPT[0][\"content\"]\n",
    "response = model.generate_content(prompt_text)\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2524345e",
   "metadata": {},
   "source": [
    "### Conversation between two models: GPT and Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "71858dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hagamos una conversación entre GPT-4o-mini y Claude-3-haiku\n",
    "# Estamos usando versiones económicas de los modelos, por lo que los costos serán mínimos\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"Eres un chatbot muy argumentativo; \\\n",
    "no estás de acuerdo con nada en la conversación y cuestionas todo de manera sarcástica.\"\n",
    "\n",
    "claude_system = \"Eres un chatbot muy educado y cortés. Intentas estar de acuerdo con \\\n",
    "lo que dice la otra persona o encontrar puntos en común. Si la otra persona discute, \\\n",
    "intentas calmarla y seguir charlando.\"\n",
    "\n",
    "gpt_messages = [\"¡Hola!\"]\n",
    "claude_messages = [\"Hola!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c7e3b313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fb1128",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "41596d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = anthropic.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b0b668c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Hola! Me alegro de saludarte. ¿Cómo estás el día de hoy? Espero que todo esté bien contigo.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44861936",
   "metadata": {},
   "source": [
    "##### To avoid having to run it manually and having to read the responses in pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a6d8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"¡Hola!\"]\n",
    "claude_messages = [\"Hola\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "\n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdc9166",
   "metadata": {},
   "source": [
    "### Conversation between three models: GPT, Gemini and Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "672fe5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = \"gpt-4o-mini\"\n",
    "gemini_model = \"models/gemini-2.5-flash\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are an eternal optimist. You always see the bright side of things and believe even \\\n",
    "simple actions have deep purpose. Keep replies under 2 sentences.\"\n",
    "\n",
    "gemini_system = \"You are a witty skeptic who questions everything. You tend to doubt grand explanations \\\n",
    "and prefer clever, sarcastic, or literal answers. Keep replies under 2 sentences.\"\n",
    "\n",
    "claude_system = \"You are a thoughtful philosopher. You consider all perspectives and enjoy finding \\\n",
    "symbolic or existential meaning in simple actions. Keep replies under 2 sentences.\"\n",
    "\n",
    "gpt_messages = [\"Hi! Todays topic for discussion is 'Why did the chicken cross the road?'\"]\n",
    "gemini_messages = [\"That's quite the topic. \"]\n",
    "claude_messages = [\"Lets begin our discussion.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "04baa1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    \n",
    "    messages = [{\"role\":\"system\", \"content\":gpt_system}]\n",
    "    \n",
    "    for gpt, gemini, claude in zip(gpt_messages, gemini_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model = gpt_model,\n",
    "        messages = messages,\n",
    "        max_tokens = 500\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a5245c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini():\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=gemini_model,\n",
    "        system_instruction=gemini_system\n",
    "    )\n",
    "    \n",
    "    # 2. Construir el historial con el formato de Gemini\n",
    "    # Formato esperado: [{'role': 'user', 'parts': ['...']}, {'role': 'model', 'parts': ['...']}]\n",
    "    chat_history = []\n",
    "    \n",
    "    # Nota: Asegúrate de que gpt_messages, gemini_messages, etc. están definidos fuera o pasados como argumentos\n",
    "    for gpt, gemini_message, claude in zip(gpt_messages, gemini_messages, claude_messages):\n",
    "        # Mensaje del Usuario\n",
    "        chat_history.append({\"role\": \"user\", \"parts\": [gpt]})\n",
    "        \n",
    "        # Respuesta del Modelo (OJO: Es 'model', no 'assistant')\n",
    "        chat_history.append({\"role\": \"model\", \"parts\": [gemini_message]})\n",
    "        \n",
    "        # Siguiente mensaje de usuario\n",
    "        chat_history.append({\"role\": \"user\", \"parts\": [claude]})\n",
    "    \n",
    "    # 3. Agregar el último mensaje (el prompt actual)\n",
    "    last_message = gpt_messages[-1] # O la variable que contenga tu último prompt\n",
    "    chat_history.append({\"role\": \"user\", \"parts\": [last_message]})\n",
    "\n",
    "    # 4. Enviar la lista completa a generate_content\n",
    "    try:\n",
    "        response = model.generate_content(chat_history)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        return f\"Error en Gemini: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c1677b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    \n",
    "    messages = []\n",
    "    \n",
    "    for gpt, gemini, claude in zip(gpt_messages, gemini_messages, claude_messages):\n",
    "        messages.append({\"role\":\"user\", \"content\":gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "        messages.append({\"role\":\"assistant\", \"content\": claude})\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    messages.append({\"role\": \"user\", \"content\": gemini_messages[-1]})\n",
    "    \n",
    "    response = anthropic.messages.create(\n",
    "        model = claude_model,\n",
    "        system = claude_system,\n",
    "        messages = messages,\n",
    "        max_tokens = MAX_TOKENS\n",
    "    )\n",
    "    return response.content[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d26423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Gemini:\\n{gemini_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT: \\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "\n",
    "    gemini_next = call_gemini()\n",
    "    print(f\"Gemini: \\n{gemini_next}\\n\")\n",
    "    gemini_messages.append(gemini_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude: \\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
