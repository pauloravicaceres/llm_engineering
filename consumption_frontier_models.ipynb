{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ad8048d",
   "metadata": {},
   "source": [
    "### Exploring and Using Frontier Model APIs\n",
    "\n",
    "##### Suggested Description\n",
    "\n",
    "This project aims to connect with and experiment with advanced language models (“Frontier Models”) through their official APIs. After using various models via their chat interfaces and working with the OpenAI API in the first week, we will now expand our scope by interacting programmatically with multiple providers, such as Anthropic, Gemini, and others, provided we have their access credentials.\n",
    "\n",
    "The goal is to send queries to different models, compare their responses, and explore the capabilities, differences, and behaviors of each API. Integration with additional providers is entirely optional, allowing each participant to customize their environment according to the tools they have access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a722840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "import anthropic as ant\n",
    "import google.generativeai as genai\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1210883",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "API_KEYS = {\n",
    "    \"OpenAI\": (\"OPENAI_API_KEY\", 8),\n",
    "    \"Anthropic\": (\"ANTHROPIC_API_KEY\", 7),\n",
    "    \"Google\": (\"GOOGLE_API_KEY\", 2),\n",
    "    \"DeepSeek\": (\"DEEPSEEK_API_KEY\", 3),\n",
    "    \"Groq\": (\"GROQ_API_KEY\", 4),\n",
    "    \"Grok\": (\"GROK_API_KEY\", 4),\n",
    "    \"OpenRouter\": (\"OPENROUTER_API_KEY\", 3),\n",
    "}\n",
    "\n",
    "keys = {label: os.getenv(env) for label, (env, _) in API_KEYS.items()}\n",
    "\n",
    "def check_key(label, prefix_len):\n",
    "    key = keys[label]\n",
    "    if key:\n",
    "        print(f\"{label} API Key exists and begins {key[:prefix_len]}\")\n",
    "    else:\n",
    "        print(f\"{label} API Key not set (optional)\")\n",
    "\n",
    "for label, (env_name, prefix_len) in API_KEYS.items():\n",
    "    check_key(label, prefix_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53141e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "\n",
    "# For Gemini, DeepSeek and Groq, we can use the OpenAI python client\n",
    "# Because Google and DeepSeek have endpoints compatible with OpenAI\n",
    "# And OpenAI allows you to change the base_url\n",
    "\n",
    "urls = {\n",
    "    \"Anthropic\": \"https://api.anthropic.com/v1/\",\n",
    "    \"Gemini\": \"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    "    \"DeepSeek\": \"https://api.deepseek.com\",\n",
    "    \"Groq\": \"https://api.groq.com/openai/v1\",\n",
    "    \"Grok\": \"https://api.x.ai/v1\",\n",
    "    \"OpenRouter\": \"https://openrouter.ai/api/v1\",\n",
    "    \"ollama\": \"http://localhost:11434/v1\",\n",
    "}\n",
    "\n",
    "openai  = OpenAI(api_key=keys[\"OpenAI\"])\n",
    "#anthropic = OpenAI(api_key=keys[\"Anthropic\"], base_url=urls[\"Anthropic\"])\n",
    "anthropic = ant.Anthropic(api_key=keys[\"Anthropic\"])\n",
    "#gemini    = OpenAI(api_key=keys[\"Google\"],    base_url=urls[\"Gemini\"])\n",
    "genai.configure(api_key=keys[\"Google\"])\n",
    "deepseek  = OpenAI(api_key=keys[\"DeepSeek\"],  base_url=urls[\"DeepSeek\"])\n",
    "groq      = OpenAI(api_key=keys[\"Groq\"],      base_url=urls[\"Groq\"])\n",
    "grok      = OpenAI(api_key=keys[\"Grok\"],      base_url=urls[\"Grok\"])\n",
    "openrouter = OpenAI(api_key=keys[\"OpenRouter\"], base_url=urls[\"OpenRouter\"])\n",
    "ollama    = OpenAI(api_key=\"ollama\", base_url=urls[\"ollama\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4243823",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 1024\n",
    "SYSTEM_PROMPT = \"You are a helpful assistant who responds in Markdown\"\n",
    "USER_PROMPT = [\n",
    "    { \"role\": \"user\", \"content\": \"How can I decide if a business problem is suitable for an LLM solution? Answer in Markdown\" }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa485cf",
   "metadata": {},
   "source": [
    "##### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93729ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_prompt = [{ \"role\": \"system\", \"content\": SYSTEM_PROMPT }] + USER_PROMPT\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=full_prompt,\n",
    "    temperature=0.2,\n",
    "    stream=True\n",
    ")\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"⏳ _Generando..._\"), display_id=True)\n",
    "for chunk in response:\n",
    "    fragment = chunk.choices[0].delta.content or \"\"\n",
    "    reply += fragment\n",
    "    clean_reply = reply\n",
    "    if clean_reply.startswith(\"```\"):\n",
    "        clean_reply = clean_reply.replace(\"```markdown\", \"\").replace(\"```\", \"\")\n",
    "    display_handle.update(Markdown(clean_reply))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86147bde",
   "metadata": {},
   "source": [
    "##### Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e12fb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = anthropic.messages.create(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    system=SYSTEM_PROMPT,\n",
    "    messages=USER_PROMPT,\n",
    "    temperature=0.2\n",
    ")\n",
    "display(Markdown(response.content[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baafc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = anthropic.messages.stream(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    system=SYSTEM_PROMPT,\n",
    "    messages=USER_PROMPT,\n",
    "    temperature=0.9\n",
    ")\n",
    "accumulated_text = \"\"\n",
    "display_handle = display(Markdown(\"⏳ Generando respuesta...\"), display_id=True)\n",
    "with response as stream:\n",
    "    for text in stream.text_stream:\n",
    "        accumulated_text += text\n",
    "        display_handle.update(Markdown(accumulated_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee946b57",
   "metadata": {},
   "source": [
    "##### Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75338a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the available Google models\n",
    "modelos = list(genai.list_models())\n",
    "\n",
    "datos_modelos = []\n",
    "for m in modelos:\n",
    "    if 'generateContent' in m.supported_generation_methods:\n",
    "        datos_modelos.append({\n",
    "            'name': m.name,\n",
    "            'display_name': m.display_name,\n",
    "            'input_limit': m.input_token_limit,\n",
    "            'output_limit': m.output_token_limit,\n",
    "            'description': m.description\n",
    "        })\n",
    "\n",
    "df_google = pd.DataFrame(datos_modelos)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "cols_a_mostrar = ['name', 'display_name', 'input_limit', 'output_limit', 'description']\n",
    "\n",
    "print(\"Google Generation Models:\")\n",
    "display(df_google[cols_a_mostrar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366163a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    model_name=\"models/gemini-2.5-flash\",\n",
    "    system_instruction=SYSTEM_PROMPT\n",
    ")\n",
    "prompt_text = USER_PROMPT[0][\"content\"]\n",
    "response = model.generate_content(prompt_text)\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2524345e",
   "metadata": {},
   "source": [
    "### Conversation between two models: GPT and Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "71858dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hagamos una conversación entre GPT-4o-mini y Claude-3-haiku\n",
    "# Estamos usando versiones económicas de los modelos, por lo que los costos serán mínimos\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"Eres un chatbot muy argumentativo; \\\n",
    "no estás de acuerdo con nada en la conversación y cuestionas todo de manera sarcástica.\"\n",
    "\n",
    "claude_system = \"Eres un chatbot muy educado y cortés. Intentas estar de acuerdo con \\\n",
    "lo que dice la otra persona o encontrar puntos en común. Si la otra persona discute, \\\n",
    "intentas calmarla y seguir charlando.\"\n",
    "\n",
    "gpt_messages = [\"¡Hola!\"]\n",
    "claude_messages = [\"Hola!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c7e3b313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fb1128",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "41596d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = anthropic.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b0b668c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Hola! Me alegro de saludarte. ¿Cómo estás el día de hoy? Espero que todo esté bien contigo.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44861936",
   "metadata": {},
   "source": [
    "##### To avoid having to run it manually and having to read the responses in pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a6d8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"¡Hola!\"]\n",
    "claude_messages = [\"Hola\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "\n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdc9166",
   "metadata": {},
   "source": [
    "### Conversation between three models: GPT, Gemini and Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "672fe5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = \"gpt-4o-mini\"\n",
    "gemini_model = \"models/gemini-2.5-flash\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are an eternal optimist. You always see the bright side of things and believe even \\\n",
    "simple actions have deep purpose. Keep replies under 2 sentences.\"\n",
    "\n",
    "gemini_system = \"You are a witty skeptic who questions everything. You tend to doubt grand explanations \\\n",
    "and prefer clever, sarcastic, or literal answers. Keep replies under 2 sentences.\"\n",
    "\n",
    "claude_system = \"You are a thoughtful philosopher. You consider all perspectives and enjoy finding \\\n",
    "symbolic or existential meaning in simple actions. Keep replies under 2 sentences.\"\n",
    "\n",
    "gpt_messages = [\"Hi! Todays topic for discussion is 'Why did the chicken cross the road?'\"]\n",
    "gemini_messages = [\"That's quite the topic. \"]\n",
    "claude_messages = [\"Lets begin our discussion.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "04baa1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    \n",
    "    messages = [{\"role\":\"system\", \"content\":gpt_system}]\n",
    "    \n",
    "    for gpt, gemini, claude in zip(gpt_messages, gemini_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model = gpt_model,\n",
    "        messages = messages,\n",
    "        max_tokens = 500\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a5245c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini():\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=gemini_model,\n",
    "        system_instruction=gemini_system\n",
    "    )\n",
    "    \n",
    "    # 2. Construir el historial con el formato de Gemini\n",
    "    # Formato esperado: [{'role': 'user', 'parts': ['...']}, {'role': 'model', 'parts': ['...']}]\n",
    "    chat_history = []\n",
    "    \n",
    "    # Nota: Asegúrate de que gpt_messages, gemini_messages, etc. están definidos fuera o pasados como argumentos\n",
    "    for gpt, gemini_message, claude in zip(gpt_messages, gemini_messages, claude_messages):\n",
    "        # Mensaje del Usuario\n",
    "        chat_history.append({\"role\": \"user\", \"parts\": [gpt]})\n",
    "        \n",
    "        # Respuesta del Modelo (OJO: Es 'model', no 'assistant')\n",
    "        chat_history.append({\"role\": \"model\", \"parts\": [gemini_message]})\n",
    "        \n",
    "        # Siguiente mensaje de usuario\n",
    "        chat_history.append({\"role\": \"user\", \"parts\": [claude]})\n",
    "    \n",
    "    # 3. Agregar el último mensaje (el prompt actual)\n",
    "    last_message = gpt_messages[-1] # O la variable que contenga tu último prompt\n",
    "    chat_history.append({\"role\": \"user\", \"parts\": [last_message]})\n",
    "\n",
    "    # 4. Enviar la lista completa a generate_content\n",
    "    try:\n",
    "        response = model.generate_content(chat_history)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        return f\"Error en Gemini: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c1677b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    \n",
    "    messages = []\n",
    "    \n",
    "    for gpt, gemini, claude in zip(gpt_messages, gemini_messages, claude_messages):\n",
    "        messages.append({\"role\":\"user\", \"content\":gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "        messages.append({\"role\":\"assistant\", \"content\": claude})\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    messages.append({\"role\": \"user\", \"content\": gemini_messages[-1]})\n",
    "    \n",
    "    response = anthropic.messages.create(\n",
    "        model = claude_model,\n",
    "        system = claude_system,\n",
    "        messages = messages,\n",
    "        max_tokens = MAX_TOKENS\n",
    "    )\n",
    "    return response.content[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d26423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Gemini:\\n{gemini_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT: \\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "\n",
    "    gemini_next = call_gemini()\n",
    "    print(f\"Gemini: \\n{gemini_next}\\n\")\n",
    "    gemini_messages.append(gemini_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude: \\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
